{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pansharpening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Band   Resolution Shape           Valid Pixels (%)  \n",
      "------------------------------------------------------------\n",
      "B01    60.0       (1830, 1830)    100.00            \n",
      "B02    10.0       (10980, 10980)  100.00            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     20\u001b[39m             valid_pct = \u001b[32m100\u001b[39m * valid_pixels / total_pixels\n\u001b[32m     22\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mband_code\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc.res[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<10.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg.shape\u001b[38;5;132;01m!s:\u001b[39;00m\u001b[33m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_pct\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<18.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mcheck_band_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/S2A_MSIL1C_20210925T092031_N0500_R093_T34SEJ_20230118T233535.SAFE/GRANULE/L1C_T34SEJ_A032694_20210925T092343/IMG_DATA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m check_band_coverage(\u001b[33m\"\u001b[39m\u001b[33mdata/S2A_MSIL1C_20210925T092031_N0500_R093_T34SFJ_20230118T233535.SAFE/GRANULE/L1C_T34SFJ_A032694_20210925T092343/IMG_DATA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m check_band_coverage(\u001b[33m\"\u001b[39m\u001b[33mdata/S2A_MSIL1C_20210925T092031_N0500_R093_T34TEK_20230118T233535.SAFE/GRANULE/L1C_T34TEK_A032694_20210925T092343/IMG_DATA\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcheck_band_coverage\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m     14\u001b[39m path = os.path.join(directory, fname)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m rasterio.open(path) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     img = \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     total_pixels = img.size\n\u001b[32m     19\u001b[39m     valid_pixels = np.count_nonzero(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mrasterio/_io.pyx:644\u001b[39m, in \u001b[36mrasterio._io.DatasetReaderBase.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mrasterio/_io.pyx:969\u001b[39m, in \u001b[36mrasterio._io.DatasetReaderBase._read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mrasterio/_io.pyx:199\u001b[39m, in \u001b[36mrasterio._io.io_multi_band\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, typ, value, traceback):\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def check_band_coverage(directory):\n",
    "    print(f\"\\n{'Band':<6} {'Resolution':<10} {'Shape':<15} {'Valid Pixels (%)':<18}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for fname in sorted(os.listdir(directory)):\n",
    "        if not fname.endswith('.jp2') or '_TCI' in fname:\n",
    "            continue\n",
    "\n",
    "        band_code = fname.split('_')[-1].replace('.jp2', '')\n",
    "        path = os.path.join(directory, fname)\n",
    "\n",
    "        with rasterio.open(path) as src:\n",
    "            img = src.read(1)\n",
    "            total_pixels = img.size\n",
    "            valid_pixels = np.count_nonzero(img)\n",
    "            valid_pct = 100 * valid_pixels / total_pixels\n",
    "\n",
    "            print(f\"{band_code:<6} {src.res[0]:<10.1f} {img.shape!s:<15} {valid_pct:<18.2f}\")\n",
    "\n",
    "\n",
    "check_band_coverage(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34SEJ_20230118T233535.SAFE/GRANULE/L1C_T34SEJ_A032694_20210925T092343/IMG_DATA\")\n",
    "check_band_coverage(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34SFJ_20230118T233535.SAFE/GRANULE/L1C_T34SFJ_A032694_20210925T092343/IMG_DATA\")\n",
    "check_band_coverage(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34TEK_20230118T233535.SAFE/GRANULE/L1C_T34TEK_A032694_20210925T092343/IMG_DATA\")\n",
    "check_band_coverage(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34TFK_20230118T233535.SAFE/GRANULE/L1C_T34TFK_A032694_20210925T092343/IMG_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS_10M = ['B02', 'B03', 'B04', 'B08']\n",
    "BANDS_20M = ['B05', 'B06', 'B07', 'B8A', 'B11', 'B12']\n",
    "BANDS_60M = ['B01', 'B09', 'B10']\n",
    "ALL_BANDS = BANDS_10M + BANDS_20M + BANDS_60M\n",
    "TARGET_SHAPE = (10980, 10980)\n",
    "\n",
    "def pansharpen_to_10m_and_save(directory, output_tiff=\"pansharpened.tif\"):\n",
    "    band_paths = {}\n",
    "    for fname in os.listdir(directory):\n",
    "        if fname.endswith('.jp2') and '_TCI' not in fname:\n",
    "            band_code = fname.split('_')[-1].replace('.jp2', '')\n",
    "            if band_code in ALL_BANDS:\n",
    "                band_paths[band_code] = os.path.join(directory, fname)\n",
    "\n",
    "    missing = [b for b in ALL_BANDS if b not in band_paths]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing bands: {missing}\")\n",
    "\n",
    "    # Use metadata from a 10m band as reference\n",
    "    with rasterio.open(band_paths[BANDS_10M[0]]) as ref_src:\n",
    "        reference_meta = ref_src.meta.copy()\n",
    "        reference_meta.update({\n",
    "            \"count\": len(ALL_BANDS),\n",
    "            \"height\": TARGET_SHAPE[0],\n",
    "            \"width\": TARGET_SHAPE[1],\n",
    "            \"driver\": \"GTiff\"\n",
    "        })\n",
    "\n",
    "    # Order bands as requested\n",
    "    ordered_bands = ordered_bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
    "    print(ordered_bands)\n",
    "\n",
    "    # Create output TIFF and write one band at a time\n",
    "    with rasterio.open(output_tiff, \"w\", **reference_meta) as dst:\n",
    "        for idx, band in enumerate(ordered_bands, start=1):\n",
    "            print(idx, band)\n",
    "            with rasterio.open(band_paths[band]) as src:\n",
    "                img = src.read(1)\n",
    "\n",
    "                if band in BANDS_10M:\n",
    "                    sharpened = img  # No resizing needed\n",
    "                else:\n",
    "                    sharpened = cv2.resize(\n",
    "                        img,\n",
    "                        (TARGET_SHAPE[1], TARGET_SHAPE[0]),\n",
    "                        interpolation=cv2.INTER_CUBIC\n",
    "                    )\n",
    "\n",
    "                dst.write(sharpened, idx)\n",
    "\n",
    "    print(f\"\\nSaved pansharpened image to: {output_tiff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
      "1 B01\n",
      "2 B02\n",
      "3 B03\n",
      "4 B04\n",
      "5 B05\n",
      "6 B06\n",
      "7 B07\n",
      "8 B08\n",
      "9 B8A\n",
      "10 B09\n",
      "11 B10\n",
      "12 B11\n",
      "13 B12\n",
      "\n",
      "Saved pansharpened image to: data/T34SEJ_pansharpened.tif\n",
      "['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
      "1 B01\n",
      "2 B02\n",
      "3 B03\n",
      "4 B04\n",
      "5 B05\n",
      "6 B06\n",
      "7 B07\n",
      "8 B08\n",
      "9 B8A\n",
      "10 B09\n",
      "11 B10\n",
      "12 B11\n",
      "13 B12\n",
      "\n",
      "Saved pansharpened image to: data/T34SFJ_pansharpened.tif\n",
      "['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
      "1 B01\n",
      "2 B02\n",
      "3 B03\n",
      "4 B04\n",
      "5 B05\n",
      "6 B06\n",
      "7 B07\n",
      "8 B08\n",
      "9 B8A\n",
      "10 B09\n",
      "11 B10\n",
      "12 B11\n",
      "13 B12\n",
      "\n",
      "Saved pansharpened image to: data/T34TEK_pansharpened.tif\n",
      "['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
      "1 B01\n",
      "2 B02\n",
      "3 B03\n",
      "4 B04\n",
      "5 B05\n",
      "6 B06\n",
      "7 B07\n",
      "8 B08\n",
      "9 B8A\n",
      "10 B09\n",
      "11 B10\n",
      "12 B11\n",
      "13 B12\n",
      "\n",
      "Saved pansharpened image to: data/T34TFK_pansharpened.tif\n"
     ]
    }
   ],
   "source": [
    "pansharpen_to_10m_and_save(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34SEJ_20230118T233535.SAFE/GRANULE/L1C_T34SEJ_A032694_20210925T092343/IMG_DATA\", output_tiff=\"data/T34SEJ_pansharpened.tif\")\n",
    "pansharpen_to_10m_and_save(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34SFJ_20230118T233535.SAFE/GRANULE/L1C_T34SFJ_A032694_20210925T092343/IMG_DATA\", output_tiff=\"data/T34SFJ_pansharpened.tif\")\n",
    "pansharpen_to_10m_and_save(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34TEK_20230118T233535.SAFE/GRANULE/L1C_T34TEK_A032694_20210925T092343/IMG_DATA\", output_tiff=\"data/T34TEK_pansharpened.tif\")\n",
    "pansharpen_to_10m_and_save(\"data/S2A_MSIL1C_20210925T092031_N0500_R093_T34TFK_20230118T233535.SAFE/GRANULE/L1C_T34TFK_A032694_20210925T092343/IMG_DATA\", output_tiff=\"data/T34TFK_pansharpened.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack shape: (13, 10980, 10980)\n",
      "Stack shape: (13, 10980, 10980)\n",
      "Stack shape: (13, 10980, 10980)\n",
      "Stack shape: (13, 10980, 10980)\n"
     ]
    }
   ],
   "source": [
    "def read_pansharpened_tiff(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        stack = src.read()  # shape will be (bands, height, width)\n",
    "        print(f\"Stack shape: {stack.shape}\")\n",
    "    return stack\n",
    "\n",
    "_ = read_pansharpened_tiff(\"data/T34SEJ_pansharpened.tif\")\n",
    "_ = read_pansharpened_tiff(\"data/T34SFJ_pansharpened.tif\")\n",
    "_ = read_pansharpened_tiff(\"data/T34TEK_pansharpened.tif\")\n",
    "_ = read_pansharpened_tiff(\"data/T34TFK_pansharpened.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth bounds:\n",
      "BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Ground truth CRS: EPSG:4326\n",
      "\n",
      "WARNING: CRS mismatch between data/T34SEJ_pansharpened.tif and ground truth!\n",
      "data/T34SEJ_pansharpened.tif: ❌ Not Covered\n",
      "WARNING: CRS mismatch between data/T34SFJ_pansharpened.tif and ground truth!\n",
      "data/T34SFJ_pansharpened.tif: ❌ Not Covered\n",
      "WARNING: CRS mismatch between data/T34TEK_pansharpened.tif and ground truth!\n",
      "data/T34TEK_pansharpened.tif: ❌ Not Covered\n",
      "WARNING: CRS mismatch between data/T34TFK_pansharpened.tif and ground truth!\n",
      "data/T34TFK_pansharpened.tif: ❌ Not Covered\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterio.coords import BoundingBox\n",
    "\n",
    "def get_bounds_and_crs(tif_path):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        return src.bounds, src.crs\n",
    "\n",
    "# Paths to your pansharpened tiles\n",
    "tile_paths = [\n",
    "    \"data/T34SEJ_pansharpened.tif\",\n",
    "    \"data/T34SFJ_pansharpened.tif\",\n",
    "    \"data/T34TEK_pansharpened.tif\",\n",
    "    \"data/T34TFK_pansharpened.tif\",\n",
    "]\n",
    "\n",
    "# Path to your ground truth .tif file\n",
    "ground_truth_path = \"data/GBDA24_ex2_ref_data.tif\"\n",
    "\n",
    "# Get bounds and CRS of the ground truth\n",
    "gt_bounds, gt_crs = get_bounds_and_crs(ground_truth_path)\n",
    "\n",
    "print(f\"Ground truth bounds:\\n{gt_bounds}\")\n",
    "print(f\"Ground truth CRS: {gt_crs}\\n\")\n",
    "\n",
    "# Check coverage for each tile\n",
    "for path in tile_paths:\n",
    "    tile_bounds, tile_crs = get_bounds_and_crs(path)\n",
    "    \n",
    "    # If CRS don't match, you'd need to reproject — for now, we assume same CRS\n",
    "    if tile_crs != gt_crs:\n",
    "        print(f\"WARNING: CRS mismatch between {path} and ground truth!\")\n",
    "    \n",
    "    # Check if tile is fully within ground truth\n",
    "    covered = (\n",
    "        tile_bounds.left   >= gt_bounds.left and\n",
    "        tile_bounds.right  <= gt_bounds.right and\n",
    "        tile_bounds.bottom >= gt_bounds.bottom and\n",
    "        tile_bounds.top    <= gt_bounds.top\n",
    "    )\n",
    "\n",
    "    status = \"✅ Covered\" if covered else \"❌ Not Covered\"\n",
    "    print(f\"{path}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Info: {'size': (21424, 13572), 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'gsd': (8.333333333333333e-05, 8.333333333333333e-05), 'bounds': BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075), 'transform': Affine(8.333333333333333e-05, 0.0, 21.331833333333332,\n",
      "       0.0, -8.333333333333333e-05, 40.26075), 'band_count': 1}\n",
      "Sentinel-2 Info for file data/T34SEJ_pansharpened.tif: {'size': (10980, 10980), 'crs': CRS.from_wkt('PROJCS[\"WGS 84 / UTM zone 34N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",21],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32634\"]]'), 'gsd': (10.0, 10.0), 'bounds': BoundingBox(left=499980.0, bottom=4290240.0, right=609780.0, top=4400040.0), 'transform': Affine(10.0, 0.0, 499980.0,\n",
      "       0.0, -10.0, 4400040.0), 'band_count': 13}\n",
      "Sentinel-2 Info for file data/T34SFJ_pansharpened.tif: {'size': (10980, 10980), 'crs': CRS.from_wkt('PROJCS[\"WGS 84 / UTM zone 34N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",21],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32634\"]]'), 'gsd': (10.0, 10.0), 'bounds': BoundingBox(left=600000.0, bottom=4290240.0, right=709800.0, top=4400040.0), 'transform': Affine(10.0, 0.0, 600000.0,\n",
      "       0.0, -10.0, 4400040.0), 'band_count': 13}\n",
      "Sentinel-2 Info for file data/T34TEK_pansharpened.tif: {'size': (10980, 10980), 'crs': CRS.from_wkt('PROJCS[\"WGS 84 / UTM zone 34N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",21],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32634\"]]'), 'gsd': (10.0, 10.0), 'bounds': BoundingBox(left=499980.0, bottom=4390200.0, right=609780.0, top=4500000.0), 'transform': Affine(10.0, 0.0, 499980.0,\n",
      "       0.0, -10.0, 4500000.0), 'band_count': 13}\n",
      "Sentinel-2 Info for file data/T34TFK_pansharpened.tif: {'size': (10980, 10980), 'crs': CRS.from_wkt('PROJCS[\"WGS 84 / UTM zone 34N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",21],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32634\"]]'), 'gsd': (10.0, 10.0), 'bounds': BoundingBox(left=600000.0, bottom=4390200.0, right=709800.0, top=4500000.0), 'transform': Affine(10.0, 0.0, 600000.0,\n",
      "       0.0, -10.0, 4500000.0), 'band_count': 13}\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "\n",
    "def get_tiff_info(filepath):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        # Image size (width, height)\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "        # CRS (Coordinate Reference System)\n",
    "        crs = src.crs\n",
    "        # GSD (Ground Sample Distance)\n",
    "        pixel_size_x = src.transform[0]  # width of a pixel\n",
    "        pixel_size_y = -src.transform[4]  # height of a pixel\n",
    "        # Get bounds\n",
    "        bounds = src.bounds\n",
    "        # Get transform\n",
    "        transform = src.transform\n",
    "        # Get number of bands\n",
    "        band_count = src.count\n",
    "        info = {\n",
    "            'size': (width, height),\n",
    "            'crs': crs,\n",
    "            'gsd': (pixel_size_x, pixel_size_y),\n",
    "            'bounds': bounds,\n",
    "            'transform': transform,\n",
    "            'band_count': band_count\n",
    "        }\n",
    "    return info\n",
    "\n",
    "# Example usage:\n",
    "ground_truth_path = \"data/GBDA24_ex2_ref_data.tif\"\n",
    "sentinel_paths = [\n",
    "    \"data/T34SEJ_pansharpened.tif\",\n",
    "    \"data/T34SFJ_pansharpened.tif\",\n",
    "    \"data/T34TEK_pansharpened.tif\",\n",
    "    \"data/T34TFK_pansharpened.tif\"\n",
    "]\n",
    "\n",
    "# Get information for the ground truth and Sentinel-2 files\n",
    "ground_truth_info = get_tiff_info(ground_truth_path)\n",
    "sentinel_infos = [get_tiff_info(path) for path in sentinel_paths]\n",
    "\n",
    "print(\"Ground Truth Info:\", ground_truth_info)\n",
    "for idx, info in enumerate(sentinel_infos):\n",
    "    print(f\"Sentinel-2 Info for file {sentinel_paths[idx]}:\", info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojected Sentinel-2 file saved as data/T34SEJ_pansharpened_reprojected_to_4326.tif\n",
      "Reprojected Sentinel-2 file saved as data/T34SFJ_pansharpened_reprojected_to_4326.tif\n",
      "Reprojected Sentinel-2 file saved as data/T34TEK_pansharpened_reprojected_to_4326.tif\n",
      "Reprojected Sentinel-2 file saved as data/T34TFK_pansharpened_reprojected_to_4326.tif\n"
     ]
    }
   ],
   "source": [
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "def reproject_raster(src_path, dst_path, dst_crs):\n",
    "    \"\"\"\n",
    "    Reproject a raster to a new CRS.\n",
    "    \"\"\"\n",
    "    with rasterio.open(src_path) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "        with rasterio.open(dst_path, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "\n",
    "# Reproject each Sentinel-2 file\n",
    "for idx, path in enumerate(sentinel_paths):\n",
    "    output_path = path.replace(\".tif\", \"_reprojected_to_4326.tif\")\n",
    "    reproject_raster(path, output_path, CRS.from_epsg(4326))  # Reproject to EPSG:4326\n",
    "    print(f\"Reprojected Sentinel-2 file saved as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/T34SEJ_pansharpened_reprojected_to_4326.tif\n",
      "Clipped Sentinel-2 file saved as data/T34SEJ_pansharpened_reprojected_to_4326_clipped.tif\n",
      "Processing data/T34SFJ_pansharpened_reprojected_to_4326.tif\n",
      "Clipped Sentinel-2 file saved as data/T34SFJ_pansharpened_reprojected_to_4326_clipped.tif\n",
      "Processing data/T34TEK_pansharpened_reprojected_to_4326.tif\n",
      "Clipped Sentinel-2 file saved as data/T34TEK_pansharpened_reprojected_to_4326_clipped.tif\n",
      "Processing data/T34TFK_pansharpened_reprojected_to_4326.tif\n",
      "Clipped Sentinel-2 file saved as data/T34TFK_pansharpened_reprojected_to_4326_clipped.tif\n"
     ]
    }
   ],
   "source": [
    "from rasterio.mask import mask\n",
    "\n",
    "def clip_to_bounds(src_path, bounds, output_path):\n",
    "    with rasterio.open(src_path) as src:\n",
    "        # Create a bounding box for clipping (Polygon format)\n",
    "        left, bottom, right, top = bounds\n",
    "        geo = {\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[\n",
    "                (left, bottom),\n",
    "                (right, bottom),\n",
    "                (right, top),\n",
    "                (left, top),\n",
    "                (left, bottom)\n",
    "            ]]\n",
    "        }\n",
    "        # Mask the raster to the bounds of the ground truth\n",
    "        out_image, out_transform = mask(src, [geo], crop=True)  # Ensure this line is using the mask function correctly\n",
    "        \n",
    "        # Update metadata\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"count\": out_image.shape[0],  # Set number of bands correctly\n",
    "            \"crs\": src.crs,\n",
    "            \"transform\": out_transform,\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2]\n",
    "        })\n",
    "        \n",
    "        # Save the clipped image (multi-band image)\n",
    "        with rasterio.open(output_path, 'w', **out_meta) as dest:\n",
    "            for i in range(out_image.shape[0]):\n",
    "                dest.write(out_image[i], i + 1)\n",
    "\n",
    "# Clip each reprojected Sentinel-2 file\n",
    "for idx, path in enumerate(sentinel_paths):\n",
    "    reprojected_path = path.replace(\".tif\", \"_reprojected_to_4326.tif\")\n",
    "    print(f\"Processing {reprojected_path}\")\n",
    "    output_path = reprojected_path.replace(\".tif\", \"_clipped.tif\")\n",
    "    clip_to_bounds(reprojected_path, ground_truth_info['bounds'], output_path)\n",
    "    print(f\"Clipped Sentinel-2 file saved as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "\n",
    "def clip_to_ground_truth(src_path, ground_truth_bounds, output_path):\n",
    "    with rasterio.open(src_path) as src:\n",
    "        # Ensure the bounds are slightly adjusted to match the ground truth\n",
    "        min_x, min_y, max_x, max_y = ground_truth_bounds\n",
    "        new_bounds = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "        # Clip the image\n",
    "        geo = box(*new_bounds)\n",
    "        out_image, out_transform = mask(src, [geo], crop=True)\n",
    "        \n",
    "        # Adjust metadata\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"count\": 1,  # Assuming we want a single band output\n",
    "            \"crs\": src.crs,\n",
    "            \"transform\": out_transform,\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"height\": out_image.shape[1],\n",
    "        })\n",
    "\n",
    "        # Save the clipped image\n",
    "        with rasterio.open(output_path, 'w', **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "clipped_tiles_paths = [\"data/T34SEJ_pansharpened_reprojected_to_4326_clipped.tif\",\n",
    "\"data/T34SFJ_pansharpened_reprojected_to_4326_clipped.tif\",\n",
    "\"data/T34TEK_pansharpened_reprojected_to_4326_clipped.tif\",\n",
    "\"data/T34TFK_pansharpened_reprojected_to_4326_clipped.tif\"]\n",
    "\n",
    "# Example usage for each tile\n",
    "for tile_path in clipped_tiles_paths:\n",
    "    output_path = tile_path.replace(\".tif\", \"_clipped_to_ground_truth.tif\")\n",
    "    clip_to_ground_truth(tile_path, ground_truth_info['bounds'], output_path)\n",
    "    print(f\"Clipped Sentinel-2 file saved as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alignment(ground_truth_bounds, clipped_image_path):\n",
    "    with rasterio.open(clipped_image_path) as src:\n",
    "        clipped_bounds = src.bounds  # Get bounds of the clipped Sentinel-2 image\n",
    "    \n",
    "    print(f\"Ground Truth Bounds: {ground_truth_bounds}\")\n",
    "    print(f\"Clipped Image Bounds: {clipped_bounds}\")\n",
    "    \n",
    "    # Check if the bounding boxes align (allowing a small tolerance)\n",
    "    tolerance = 0.0001  # Adjust tolerance as needed (e.g., 0.0001 degrees)\n",
    "    aligned = all(abs(ground_truth_bounds[i] - clipped_bounds[i]) <= tolerance for i in range(4))\n",
    "    \n",
    "    if aligned:\n",
    "        print(\"Ground truth and clipped image are aligned.\")\n",
    "    else:\n",
    "        print(\"Ground truth and clipped image are NOT aligned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=20.99976654562274, bottom=38.75400437503255, right=22.281313387914107, top=39.75026792656398)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=22.150872219612868, bottom=38.73595594721768, right=23.44792981139987, top=39.74439989605869)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=20.99976343610849, bottom=39.65458947552921, right=22.29831573625469, top=40.650856515329274)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=22.165665520077653, bottom=39.635881355812025, right=23.48040176606686, top=40.64479964952879)\n",
      "Ground truth and clipped image are NOT aligned.\n"
     ]
    }
   ],
   "source": [
    "check_alignment(ground_truth_info['bounds'], \"data/T34SEJ_pansharpened_reprojected_to_4326.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34SFJ_pansharpened_reprojected_to_4326.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34TEK_pansharpened_reprojected_to_4326.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34TFK_pansharpened_reprojected_to_4326.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=21.331785311912185, bottom=39.129649826904966, right=22.28131338791411, top=39.75026792656398)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=22.150872219612868, bottom=39.129688977827016, right=23.117185804832417, top=39.74439989605869)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=21.331817631906816, bottom=39.6545894755292, right=22.29831573625469, top=40.26083123470299)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=22.165665520077653, bottom=39.635881355812025, right=23.117249451187373, top=40.260836313739304)\n",
      "Ground truth and clipped image are NOT aligned.\n"
     ]
    }
   ],
   "source": [
    "check_alignment(ground_truth_info['bounds'], \"data/T34SEJ_pansharpened_reprojected_to_4326_clipped.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34SFJ_pansharpened_reprojected_to_4326_clipped.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34TEK_pansharpened_reprojected_to_4326_clipped.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34TFK_pansharpened_reprojected_to_4326_clipped.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=21.331785311912185, bottom=39.129649826904966, right=22.28131338791411, top=39.75026792656398)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=22.150872219612868, bottom=39.129688977827016, right=23.117185804832417, top=39.74439989605869)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=21.331817631906816, bottom=39.6545894755292, right=22.29831573625469, top=40.26083123470299)\n",
      "Ground truth and clipped image are NOT aligned.\n",
      "Ground Truth Bounds: BoundingBox(left=21.331833333333332, bottom=39.12975, right=23.117166666666666, top=40.26075)\n",
      "Clipped Image Bounds: BoundingBox(left=22.165665520077653, bottom=39.635881355812025, right=23.117249451187373, top=40.260836313739304)\n",
      "Ground truth and clipped image are NOT aligned.\n"
     ]
    }
   ],
   "source": [
    "check_alignment(ground_truth_info['bounds'], \"data/T34SEJ_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34SFJ_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34TEK_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\")\n",
    "check_alignment(ground_truth_info['bounds'], \"data/T34TFK_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/T34SEJ_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: Satellite image shape (13, 5989, 9163) does not match ground truth shape (1, 13572, 21424)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Generate and save the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtile_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mgenerate_dataset\u001b[39m\u001b[34m(tile_paths, ground_truth_path, output_dir)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Ensure that the satellite image and ground truth have the same shape (dimension alignment)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m satellite_image.shape != ground_truth.shape:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape mismatch: Satellite image shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msatellite_image.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoes not match ground truth shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mground_truth.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Append the images to their respective lists\u001b[39;00m\n\u001b[32m     32\u001b[39m satellite_images.append(satellite_image)\n",
      "\u001b[31mValueError\u001b[39m: Shape mismatch: Satellite image shape (13, 5989, 9163) does not match ground truth shape (1, 13572, 21424)"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to load a TIFF file and convert it to numpy array\n",
    "def load_tiff_as_array(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        return src.read()\n",
    "\n",
    "# Function to generate the dataset\n",
    "def generate_dataset(tile_paths, ground_truth_path, output_dir):\n",
    "    # Load the ground truth image\n",
    "    ground_truth = load_tiff_as_array(ground_truth_path)\n",
    "    \n",
    "    # Initialize lists to store pairs of satellite images and ground truth images\n",
    "    satellite_images = []\n",
    "    ground_truth_images = []\n",
    "    \n",
    "    # Loop through each satellite tile\n",
    "    for tile_path in tile_paths:\n",
    "        print(f\"Processing {tile_path}...\")\n",
    "        \n",
    "        # Load the satellite image tile as numpy array\n",
    "        satellite_image = load_tiff_as_array(tile_path)\n",
    "        \n",
    "        # Ensure that the satellite image and ground truth have the same shape (dimension alignment)\n",
    "        if satellite_image.shape != ground_truth.shape:\n",
    "            raise ValueError(f\"Shape mismatch: Satellite image shape {satellite_image.shape} \"\n",
    "                             f\"does not match ground truth shape {ground_truth.shape}\")\n",
    "        \n",
    "        # Append the images to their respective lists\n",
    "        satellite_images.append(satellite_image)\n",
    "        ground_truth_images.append(ground_truth)\n",
    "    \n",
    "    # Convert the lists of images to numpy arrays\n",
    "    satellite_images_np = np.array(satellite_images)\n",
    "    ground_truth_images_np = np.array(ground_truth_images)\n",
    "    \n",
    "    # Save the dataset as numpy arrays\n",
    "    np.save(os.path.join(output_dir, 'satellite_images.npy'), satellite_images_np)\n",
    "    np.save(os.path.join(output_dir, 'ground_truth_images.npy'), ground_truth_images_np)\n",
    "    \n",
    "    print(f\"Dataset saved: {output_dir}/satellite_images.npy and {output_dir}/ground_truth_images.npy\")\n",
    "\n",
    "# Example usage\n",
    "tile_paths = [\n",
    "    \"data/T34SEJ_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\",\n",
    "    \"data/T34SFJ_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\",\n",
    "    \"data/T34TEK_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\",\n",
    "    \"data/T34TFK_pansharpened_reprojected_to_4326_clipped_clipped_to_ground_truth.tif\"]\n",
    "ground_truth_path = \"data/GBDA24_ex2_ref_data.tif\"\n",
    "output_dir = \"generated_dataset\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate and save the dataset\n",
    "generate_dataset(tile_paths, ground_truth_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aligned/T34TEK_pansharpened_aligned.tif\n",
      "Processed data/aligned/T34TEK_pansharpened_aligned.tif, saved 89328181 samples\n",
      "data/aligned/T34SFJ_pansharpened_aligned.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.windows import from_bounds\n",
    "from glob import glob\n",
    "import uuid\n",
    "\n",
    "tile_folder = \"data/aligned/\"           # Folder with your aligned tiles\n",
    "reference_path = \"data/reference_repoj.tif\"         # Ground truth raster\n",
    "output_dataset = \"image_label_dataset.npz\"  # Output file\n",
    "temp_dir = \"temp_batches\"\n",
    "\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "tile_paths = glob(os.path.join(tile_folder, \"*.tif\"))\n",
    "sample_idx = 0\n",
    "\n",
    "batch_paths_X = []\n",
    "batch_paths_y = []\n",
    "\n",
    "with rasterio.open(reference_path) as ref_ds:\n",
    "    ref_data = ref_ds.read(1)\n",
    "    ref_nodata = ref_ds.nodata\n",
    "    ref_bounds = ref_ds.bounds\n",
    "\n",
    "    for tile_path in tile_paths:\n",
    "        print(tile_path)\n",
    "        with rasterio.open(tile_path) as tile_ds:\n",
    "            tile_nodata = tile_ds.nodata\n",
    "\n",
    "            # Get intersection bounds\n",
    "            intersection_bounds = (\n",
    "                max(tile_ds.bounds.left, ref_bounds.left),\n",
    "                max(tile_ds.bounds.bottom, ref_bounds.bottom),\n",
    "                min(tile_ds.bounds.right, ref_bounds.right),\n",
    "                min(tile_ds.bounds.top, ref_bounds.top)\n",
    "            )\n",
    "\n",
    "            if intersection_bounds[0] >= intersection_bounds[2] or intersection_bounds[1] >= intersection_bounds[3]:\n",
    "                continue\n",
    "\n",
    "            # Read overlapping window\n",
    "            tile_window = from_bounds(*intersection_bounds, transform=tile_ds.transform)\n",
    "            ref_window = from_bounds(*intersection_bounds, transform=ref_ds.transform)\n",
    "\n",
    "            tile_data = tile_ds.read(window=tile_window)\n",
    "            ref_crop = ref_ds.read(1, window=ref_window)\n",
    "\n",
    "            # Resize to common shape\n",
    "            H = min(tile_data.shape[1], ref_crop.shape[0])\n",
    "            W = min(tile_data.shape[2], ref_crop.shape[1])\n",
    "            tile_data = tile_data[:, :H, :W]\n",
    "            ref_crop = ref_crop[:H, :W]\n",
    "\n",
    "            tile_mask = np.all(tile_data != tile_nodata, axis=0) if tile_nodata is not None else np.ones((H, W), dtype=bool)\n",
    "            ref_mask = (ref_crop != ref_nodata) if ref_nodata is not None else np.ones((H, W), dtype=bool)\n",
    "            valid_mask = tile_mask & ref_mask\n",
    "\n",
    "            if np.count_nonzero(valid_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            pixels = tile_data[:, valid_mask].T  # (N, bands)\n",
    "            labels = ref_crop[valid_mask]        # (N,)\n",
    "\n",
    "            # Save batch\n",
    "            batch_id = uuid.uuid4().hex\n",
    "            X_path = os.path.join(temp_dir, f\"X_{batch_id}.npy\")\n",
    "            y_path = os.path.join(temp_dir, f\"y_{batch_id}.npy\")\n",
    "            np.save(X_path, pixels)\n",
    "            np.save(y_path, labels)\n",
    "\n",
    "            batch_paths_X.append(X_path)\n",
    "            batch_paths_y.append(y_path)\n",
    "\n",
    "            sample_idx += len(labels)\n",
    "            print(f\"Processed {tile_path}, saved {len(labels)} samples\")\n",
    "\n",
    "# Combine batches\n",
    "print(\"Merging all batches into final dataset...\")\n",
    "X_all = np.concatenate([np.load(p) for p in batch_paths_X], axis=0)\n",
    "y_all = np.concatenate([np.load(p) for p in batch_paths_y], axis=0)\n",
    "\n",
    "np.savez_compressed(output_dataset, X=X_all, y=y_all)\n",
    "print(f\"✅ Done. Saved final dataset: {output_dataset}\")\n",
    "print(f\"Total samples: {X_all.shape[0]}\")\n",
    "\n",
    "# Clean up temp\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aligned/T34TEK_pansharpened_aligned.tif\n",
      "data/aligned/T34SFJ_pansharpened_aligned.tif\n",
      "data/aligned/T34SEJ_pansharpened_aligned.tif\n",
      "data/aligned/T34TFK_pansharpened_aligned.tif\n",
      "\n",
      "🔁 Merging patches into final dataset...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.windows import from_bounds\n",
    "from glob import glob\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "# CONFIG\n",
    "tile_folder = \"data/aligned/\"           # Folder with your aligned tiles\n",
    "reference_path = \"data/reference_repoj.tif\"         # Ground truth raster\n",
    "output_npz = \"image_label_dataset.npz\"\n",
    "temp_dir = \"temp_patch_batches\"\n",
    "patch_size = 64\n",
    "stride = 32  # overlapping windows\n",
    "\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "tile_paths = glob(os.path.join(tile_folder, \"*.tif\"))\n",
    "X_paths, y_paths = [], []\n",
    "\n",
    "with rasterio.open(reference_path) as ref_ds:\n",
    "    ref_bounds = ref_ds.bounds\n",
    "    ref_nodata = ref_ds.nodata\n",
    "    ref_transform = ref_ds.transform\n",
    "\n",
    "    for tile_path in tile_paths:\n",
    "        print(tile_path)\n",
    "        with rasterio.open(tile_path) as tile_ds:\n",
    "            tile_nodata = tile_ds.nodata\n",
    "\n",
    "            # Find overlap region\n",
    "            intersection_bounds = (\n",
    "                max(tile_ds.bounds.left, ref_bounds.left),\n",
    "                max(tile_ds.bounds.bottom, ref_bounds.bottom),\n",
    "                min(tile_ds.bounds.right, ref_bounds.right),\n",
    "                min(tile_ds.bounds.top, ref_bounds.top)\n",
    "            )\n",
    "\n",
    "            if intersection_bounds[0] >= intersection_bounds[2] or intersection_bounds[1] >= intersection_bounds[3]:\n",
    "                continue\n",
    "\n",
    "            # Read overlapping region\n",
    "            tile_window = from_bounds(*intersection_bounds, transform=tile_ds.transform)\n",
    "            ref_window = from_bounds(*intersection_bounds, transform=ref_ds.transform)\n",
    "\n",
    "            tile_data = tile_ds.read(window=tile_window)\n",
    "            ref_crop = ref_ds.read(1, window=ref_window)\n",
    "\n",
    "            # Resize to match shape\n",
    "            H = min(tile_data.shape[1], ref_crop.shape[0])\n",
    "            W = min(tile_data.shape[2], ref_crop.shape[1])\n",
    "            tile_data = tile_data[:, :H, :W]\n",
    "            ref_crop = ref_crop[:H, :W]\n",
    "\n",
    "            # Slide patch window\n",
    "            for row in range(0, H - patch_size + 1, stride):\n",
    "                for col in range(0, W - patch_size + 1, stride):\n",
    "                    image_patch = tile_data[:, row:row + patch_size, col:col + patch_size]\n",
    "                    label_patch = ref_crop[row:row + patch_size, col:col + patch_size]\n",
    "\n",
    "                    if tile_nodata is not None:\n",
    "                        valid_image = np.all(image_patch != tile_nodata, axis=0)\n",
    "                    else:\n",
    "                        valid_image = np.ones((patch_size, patch_size), dtype=bool)\n",
    "\n",
    "                    if ref_nodata is not None:\n",
    "                        valid_label = (label_patch != ref_nodata)\n",
    "                    else:\n",
    "                        valid_label = np.ones((patch_size, patch_size), dtype=bool)\n",
    "\n",
    "                    if np.all(valid_image & valid_label):\n",
    "                        # Save patch\n",
    "                        patch_id = uuid.uuid4().hex\n",
    "                        X_path = os.path.join(temp_dir, f\"X_{patch_id}.npy\")\n",
    "                        y_path = os.path.join(temp_dir, f\"y_{patch_id}.npy\")\n",
    "                        np.save(X_path, image_patch.astype(np.float32))\n",
    "                        np.save(y_path, label_patch.astype(np.int16))\n",
    "                        X_paths.append(X_path)\n",
    "                        y_paths.append(y_path)\n",
    "\n",
    "                        #print(f\"✅ Patch saved: {patch_id}\")\n",
    "\n",
    "# Merge all patches\n",
    "print(\"\\n🔁 Merging patches into final dataset...\")\n",
    "X_all = np.stack([np.load(p) for p in X_paths])\n",
    "y_all = np.stack([np.load(p) for p in y_paths])\n",
    "np.savez_compressed(output_npz, X=X_all, y=y_all)\n",
    "print(f\"✅ Done. Saved {X_all.shape[0]} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "# Clean up temp files\n",
    "shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aligned/T34TEK_pansharpened_aligned.tif\n",
      "data/aligned/T34TFK_pansharpened_aligned.tif\n",
      "data/aligned/T34SEJ_pansharpened_aligned.tif\n",
      "data/aligned/T34SFJ_pansharpened_aligned.tif\n",
      "\n",
      "🔁 Merging 1000 patches into final dataset...\n",
      "✅ Done. Saved 1000 patches of size 128x128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.windows import from_bounds\n",
    "from glob import glob\n",
    "import uuid\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# CONFIG\n",
    "tile_folder = \"data/aligned/\"           # Folder with your aligned tiles\n",
    "reference_path = \"data/reference_repoj.tif\"  # Ground truth raster\n",
    "output_npz = \"image_label_dataset.npz\"\n",
    "temp_dir = \"temp_patch_batches\"\n",
    "patch_size = 128\n",
    "stride = 32  # overlapping windows\n",
    "MAX_PATCHES = 1000  # Cap on number of patches\n",
    "\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "tile_paths = glob(os.path.join(tile_folder, \"*.tif\"))\n",
    "random.shuffle(tile_paths)  # optional: randomize tile order\n",
    "X_paths, y_paths = [], []\n",
    "patch_counter = 0\n",
    "\n",
    "with rasterio.open(reference_path) as ref_ds:\n",
    "    ref_bounds = ref_ds.bounds\n",
    "    ref_nodata = ref_ds.nodata\n",
    "    ref_transform = ref_ds.transform\n",
    "\n",
    "    for tile_path in tile_paths:\n",
    "        print(tile_path)\n",
    "        with rasterio.open(tile_path) as tile_ds:\n",
    "            tile_nodata = tile_ds.nodata\n",
    "\n",
    "            # Find overlap region\n",
    "            intersection_bounds = (\n",
    "                max(tile_ds.bounds.left, ref_bounds.left),\n",
    "                max(tile_ds.bounds.bottom, ref_bounds.bottom),\n",
    "                min(tile_ds.bounds.right, ref_bounds.right),\n",
    "                min(tile_ds.bounds.top, ref_bounds.top)\n",
    "            )\n",
    "\n",
    "            if intersection_bounds[0] >= intersection_bounds[2] or intersection_bounds[1] >= intersection_bounds[3]:\n",
    "                continue\n",
    "\n",
    "            # Read overlapping region\n",
    "            tile_window = from_bounds(*intersection_bounds, transform=tile_ds.transform)\n",
    "            ref_window = from_bounds(*intersection_bounds, transform=ref_ds.transform)\n",
    "\n",
    "            tile_data = tile_ds.read(window=tile_window)\n",
    "            ref_crop = ref_ds.read(1, window=ref_window)\n",
    "\n",
    "            # Resize to match shape\n",
    "            H = min(tile_data.shape[1], ref_crop.shape[0])\n",
    "            W = min(tile_data.shape[2], ref_crop.shape[1])\n",
    "            tile_data = tile_data[:, :H, :W]\n",
    "            ref_crop = ref_crop[:H, :W]\n",
    "\n",
    "            # Slide patch window\n",
    "            for row in range(0, H - patch_size + 1, stride):\n",
    "                for col in range(0, W - patch_size + 1, stride):\n",
    "                    if patch_counter >= MAX_PATCHES:\n",
    "                        break\n",
    "\n",
    "                    image_patch = tile_data[:, row:row + patch_size, col:col + patch_size]\n",
    "                    label_patch = ref_crop[row:row + patch_size, col:col + patch_size]\n",
    "\n",
    "                    if tile_nodata is not None:\n",
    "                        valid_image = np.all(image_patch != tile_nodata, axis=0)\n",
    "                    else:\n",
    "                        valid_image = np.ones((patch_size, patch_size), dtype=bool)\n",
    "\n",
    "                    if ref_nodata is not None:\n",
    "                        valid_label = (label_patch != ref_nodata)\n",
    "                    else:\n",
    "                        valid_label = np.ones((patch_size, patch_size), dtype=bool)\n",
    "\n",
    "                    if np.all(valid_image & valid_label):\n",
    "                        # Save patch\n",
    "                        patch_id = uuid.uuid4().hex\n",
    "                        X_path = os.path.join(temp_dir, f\"X_{patch_id}.npy\")\n",
    "                        y_path = os.path.join(temp_dir, f\"y_{patch_id}.npy\")\n",
    "                        np.save(X_path, image_patch.astype(np.float32))\n",
    "                        np.save(y_path, label_patch.astype(np.int16))\n",
    "                        X_paths.append(X_path)\n",
    "                        y_paths.append(y_path)\n",
    "                        patch_counter += 1\n",
    "\n",
    "# Merge all patches\n",
    "print(f\"\\n🔁 Merging {patch_counter} patches into final dataset...\")\n",
    "X_all = np.stack([np.load(p) for p in X_paths])\n",
    "y_all = np.stack([np.load(p) for p in y_paths])\n",
    "\n",
    "np.savez_compressed(output_npz, X=X_all, y=y_all)\n",
    "print(f\"✅ Done. Saved {X_all.shape[0]} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "# Clean up temp files\n",
    "shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandUseDataset(Dataset):\n",
    "    def __init__(self, npz_path, transform=None):\n",
    "        data = np.load(npz_path)\n",
    "        self.X = data['X']  # Shape: (N, 13, H, W)\n",
    "        self.y = data['y']  # Shape: (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(128, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.upconv2 = DoubleConv(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.upconv1 = DoubleConv(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool1(d1))\n",
    "        b = self.bottleneck(self.pool2(d2))\n",
    "\n",
    "        u2 = self.up2(b)\n",
    "        u2 = self.upconv2(torch.cat([u2, d2], dim=1))\n",
    "        u1 = self.up1(u2)\n",
    "        u1 = self.upconv1(torch.cat([u1, d1], dim=1))\n",
    "        return self.final(u1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = LandUseDataset(\"image_label_dataset.npz\")\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_channels=13, num_classes=dataset.y.max()+1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 37.7594\n",
      "Epoch 2, Loss: 32.4856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m X, y = X.to(device), y.to(device)\n\u001b[32m      7\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m loss = criterion(logits, y)\n\u001b[32m     10\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     37\u001b[39m b = \u001b[38;5;28mself\u001b[39m.bottleneck(\u001b[38;5;28mself\u001b[39m.pool2(d2))\n\u001b[32m     39\u001b[39m u2 = \u001b[38;5;28mself\u001b[39m.up2(b)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m u2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m u1 = \u001b[38;5;28mself\u001b[39m.up1(u2)\n\u001b[32m     42\u001b[39m u1 = \u001b[38;5;28mself\u001b[39m.upconv1(torch.cat([u1, d1], dim=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mDoubleConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml_dl/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):  # or more\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
