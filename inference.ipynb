{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import UNetResNet50, check_band_coverage, pansharpen_to_10m_and_save, read_pansharpened_tiff\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " L1C_T34SEH_A030692_20210508T092338\n",
      "\n",
      "Band   Resolution Shape           Valid Pixels (%)  \n",
      "------------------------------------------------------------\n",
      "B01    60.0       (1830, 1830)    100.00            \n",
      "B02    10.0       (10980, 10980)  100.00            \n",
      "B03    10.0       (10980, 10980)  100.00            \n",
      "B04    10.0       (10980, 10980)  100.00            \n",
      "B05    20.0       (5490, 5490)    100.00            \n",
      "B06    20.0       (5490, 5490)    100.00            \n",
      "B07    20.0       (5490, 5490)    100.00            \n",
      "B08    10.0       (10980, 10980)  100.00            \n",
      "B09    60.0       (1830, 1830)    100.00            \n",
      "B10    60.0       (1830, 1830)    100.00            \n",
      "B11    20.0       (5490, 5490)    100.00            \n",
      "B12    20.0       (5490, 5490)    100.00            \n",
      "B8A    20.0       (5490, 5490)    100.00            \n"
     ]
    }
   ],
   "source": [
    "check_band_coverage(\"data/S2A_MSIL1C_20210508T092031_N0500_R093_T34SEH_20230303T013318.SAFE/GRANULE/L1C_T34SEH_A030692_20210508T092338/IMG_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n",
      "1 B01\n",
      "2 B02\n",
      "3 B03\n",
      "4 B04\n",
      "5 B05\n",
      "6 B06\n",
      "7 B07\n",
      "8 B08\n",
      "9 B8A\n",
      "10 B09\n",
      "11 B10\n",
      "12 B11\n",
      "13 B12\n",
      "\n",
      "Saved pansharpened image to: data/T34SEH_pansharpened.tif\n"
     ]
    }
   ],
   "source": [
    "pansharpen_to_10m_and_save(\"data/S2A_MSIL1C_20210508T092031_N0500_R093_T34SEH_20230303T013318.SAFE/GRANULE/L1C_T34SEH_A030692_20210508T092338/IMG_DATA\", output_tiff=\"data/T34SEH_pansharpened.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_tile(model, tile_array, device, patch_size=128, stride=64, num_classes=10):\n",
    "    \"\"\"\n",
    "    Run sliding window inference on a large tile.\n",
    "\n",
    "    Args:\n",
    "        model: Trained UNetResNet50 model.\n",
    "        tile_array: Numpy array of shape (13, H, W).\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        patch_size: Size of each square patch (default 128).\n",
    "        stride: Step size between patches (default 64).\n",
    "        num_classes: Number of classes in output mask.\n",
    "\n",
    "    Returns:\n",
    "        final_prediction: 2D numpy array (H, W) of predicted class IDs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    _, H, W = tile_array.shape\n",
    "    output_probs = np.zeros((num_classes, H, W), dtype=np.float32)\n",
    "    count = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(range(0, H - patch_size + 1, stride)):\n",
    "            for col in range(0, W - patch_size + 1, stride):\n",
    "                patch = tile_array[:, row:row + patch_size, col:col + patch_size]\n",
    "                patch_tensor = torch.from_numpy(patch).unsqueeze(0).to(device).float()  # (1, 13, 128, 128)\n",
    "\n",
    "                logits = model(patch_tensor)  # (1, num_classes, 128, 128)\n",
    "                probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()  # (num_classes, 128, 128)\n",
    "\n",
    "                output_probs[:, row:row + patch_size, col:col + patch_size] += probs\n",
    "                count[row:row + patch_size, col:col + patch_size] += 1\n",
    "\n",
    "    avg_probs = output_probs / count.clip(min=1e-8)  # Avoid divide by zero\n",
    "    final_prediction = np.argmax(avg_probs, axis=0).astype(np.uint8)\n",
    "    np.save(\"predicted_mask.npy\", final_prediction)\n",
    "    print(\"Prediction Saved\")\n",
    "    return final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack shape: (13, 10979, 10979)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 10979, 10979)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile = read_pansharpened_tiff(\"data/T34SEH_pansharpened_aligned.tif\")\n",
    "tile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack shape: (13, 10980, 10980)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 10980, 10980)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile = read_pansharpened_tiff(\"data/T34SEH_pansharpened.tif\")\n",
    "tile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_full_tile_streaming(model, tiff_path, device, patch_size=128, stride=64, num_classes=8, output_path=\"predicted_mask.npy\"):\n",
    "    \"\"\"\n",
    "    Perform sliding window inference on a large TIFF tile using disk streaming.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        tiff_path: Path to the input GeoTIFF.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "        patch_size: Size of square patches.\n",
    "        stride: Step size between patches.\n",
    "        num_classes: Number of classes.\n",
    "        output_path: Where to save the prediction.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        H, W = src.height, src.width\n",
    "        output_probs = np.zeros((num_classes, H, W), dtype=np.float32)\n",
    "        count = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for row in tqdm(range(0, H - patch_size + 1, stride)):\n",
    "                for col in range(0, W - patch_size + 1, stride):\n",
    "                    # Read patch from file: (bands, patch_size, patch_size)\n",
    "                    patch = src.read(window=rasterio.windows.Window(col, row, patch_size, patch_size))\n",
    "                    if patch.shape[1] != patch_size or patch.shape[2] != patch_size:\n",
    "                        continue  # Skip edge cases if you don't pad\n",
    "\n",
    "                    # Normalize and convert to torch tensor\n",
    "                    patch = patch.astype(np.float32) / 10000.0\n",
    "                    patch = np.clip(patch, 0.0, 1.0)\n",
    "                    patch_tensor = torch.from_numpy(patch).unsqueeze(0).to(device)  # (1, 13, H, W)\n",
    "\n",
    "                    logits = model(patch_tensor)\n",
    "                    probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                    output_probs[:, row:row + patch_size, col:col + patch_size] += probs\n",
    "                    count[row:row + patch_size, col:col + patch_size] += 1\n",
    "\n",
    "        avg_probs = output_probs / np.clip(count, 1e-8, None)\n",
    "        final_prediction = np.argmax(avg_probs, axis=0).astype(np.uint8)\n",
    "        np.save(output_path, final_prediction)\n",
    "        print(f\"Prediction saved to {output_path}\")\n",
    "        return final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/pattrec1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/alex/miniconda3/envs/pattrec1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 170/170 [04:33<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to predicted_mask.npy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = UNetResNet50(num_classes=8).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model_lr_0.0001.pt\", map_location=device, weights_only=True))\n",
    "pred_mask = predict_full_tile_streaming(model, \"data/T34SEH_pansharpened.tif\", device, patch_size=128, stride=64, num_classes=8, output_path=\"predicted_mask.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoTIFF saved to: pred_mask.tif\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "def save_prediction_geotiff(prediction, reference_tiff_path, output_path):\n",
    "    \"\"\"\n",
    "    Save the predicted mask as a GeoTIFF using the georeferencing info from the original tile.\n",
    "\n",
    "    Args:\n",
    "        prediction (np.ndarray): 2D array of predicted class IDs (H, W).\n",
    "        reference_tiff_path (str): Path to the original input GeoTIFF (e.g., Sentinel-2 tile).\n",
    "        output_path (str): Path to save the output GeoTIFF.\n",
    "    \"\"\"\n",
    "    with rasterio.open(reference_tiff_path) as src:\n",
    "        profile = src.profile\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "\n",
    "    # Update profile for single-band uint8 mask\n",
    "    profile.update({\n",
    "        'driver': 'GTiff',\n",
    "        'height': prediction.shape[0],\n",
    "        'width': prediction.shape[1],\n",
    "        'count': 1,\n",
    "        'dtype': 'uint8',\n",
    "        'transform': transform,\n",
    "        'crs': crs\n",
    "    })\n",
    "\n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(prediction, 1)\n",
    "\n",
    "    print(f\"GeoTIFF saved to: {output_path}\")\n",
    "\n",
    "\n",
    "save_prediction_geotiff(pred_mask, \"data/T34SEH_pansharpened.tif\", \"pred_mask.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6], dtype=uint8),\n",
       " array([10, 20, 30, 40, 50, 60, 80], dtype=uint8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2idx = {10: 0, 20: 1, 30: 2, 40: 3, 50: 4, 60: 5, 80: 6, 90: 7}\n",
    "idx2id = {v: k for k, v in id2idx.items()}\n",
    "\n",
    "# remapping\n",
    "remapped_mask = np.vectorize(idx2id.get)(pred_mask).astype(np.uint8)\n",
    "np.unique(pred_mask), np.unique(remapped_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def load_reference_mask(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        mask = src.read(1)  # Read first band\n",
    "    return mask\n",
    "\n",
    "\n",
    "def compute_metrics(pred_mask, true_mask, class_ids):\n",
    "    \"\"\"\n",
    "    Evaluate segmentation results.\n",
    "\n",
    "    Args:\n",
    "        pred_mask: 2D numpy array of predicted class IDs.\n",
    "        true_mask: 2D numpy array of true class IDs.\n",
    "        class_ids: List of valid class IDs (e.g., [10, 20, 30, ..., 90]).\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with pixel accuracy, per-class IoU, and mean IoU.\n",
    "    \"\"\"\n",
    "    # Flatten for easier computation\n",
    "    print(\"Prediction shape:\", pred_mask.shape)\n",
    "    print(\"Ground truth shape:\", true_mask.shape)\n",
    "\n",
    "    pred = pred_mask.flatten()\n",
    "    true = true_mask.flatten()\n",
    "\n",
    "    # Mask out invalid labels (e.g., background or nodata in ground truth)\n",
    "    valid = np.isin(true, class_ids)\n",
    "    pred = pred[valid]\n",
    "    true = true[valid]\n",
    "\n",
    "    # Confusion matrix using class IDs directly\n",
    "    cm = confusion_matrix(true, pred, labels=class_ids)\n",
    "\n",
    "    intersection = np.diag(cm)\n",
    "    ground_truth_set = cm.sum(axis=1)\n",
    "    predicted_set = cm.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "\n",
    "    iou_per_class = intersection / np.clip(union, 1e-8, None)\n",
    "    mean_iou = np.mean(iou_per_class)\n",
    "    pixel_accuracy = np.sum(intersection) / np.clip(np.sum(cm), 1e-8, None)\n",
    "\n",
    "    return {\n",
    "        \"pixel_accuracy\": pixel_accuracy,\n",
    "        \"iou_per_class\": dict(zip(class_ids, iou_per_class)),\n",
    "        \"mean_iou\": mean_iou\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (10980, 10980)\n",
      "Ground truth shape: (10980, 10980)\n",
      "Pixel Accuracy: 0.7508253886707643\n",
      "Per-Class IoU: {10: np.float64(0.7258283394437622), 20: np.float64(0.04968237770667938), 30: np.float64(0.18715757604524064), 40: np.float64(0.2584323683147882), 50: np.float64(0.1721664531071201), 60: np.float64(0.04611749934373815), 80: np.float64(0.956728629252614), 90: np.float64(0.0)}\n",
      "Mean IoU: 0.29951415540174287\n"
     ]
    }
   ],
   "source": [
    "true_mask = load_reference_mask(\"data/GBDA24_ex2_34SEH_ref_data.tif\")\n",
    "\n",
    "# List of all valid class IDs\n",
    "class_ids = [10, 20, 30, 40, 50, 60, 80, 90]\n",
    "\n",
    "# Evaluate\n",
    "metrics = compute_metrics(remapped_mask, true_mask, class_ids)\n",
    "print(\"Pixel Accuracy:\", metrics[\"pixel_accuracy\"])\n",
    "print(\"Per-Class IoU:\", metrics[\"iou_per_class\"])\n",
    "print(\"Mean IoU:\", metrics[\"mean_iou\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattrec1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
