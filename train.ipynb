{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandUseDataset(Dataset):\n",
    "    def __init__(self, npz_path, transform=None):\n",
    "        data = np.load(npz_path)\n",
    "        self.X = np.clip(data['X'] / 10000.0, 0.0, 1.0) # Shape: (N, 13, H, W)\n",
    "        self.y = data['y']  # Shape: (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=13):\n",
    "        super(UNetResNet18, self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet18\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Override the first conv layer to accept 13 input channels\n",
    "        self.encoder_conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder_bn1 = resnet.bn1\n",
    "        self.encoder_relu = resnet.relu\n",
    "        self.encoder_maxpool = resnet.maxpool\n",
    "\n",
    "        # ResNet layers\n",
    "        self.encoder_layer1 = resnet.layer1  # 64 -> 64\n",
    "        self.encoder_layer2 = resnet.layer2  # 64 -> 128\n",
    "        self.encoder_layer3 = resnet.layer3  # 128 -> 256\n",
    "        self.encoder_layer4 = resnet.layer4  # 256 -> 512\n",
    "\n",
    "        # Decoder part (upsampling + skip connections)\n",
    "        self.upconv4 = self._upsample(512, 256)\n",
    "        self.upconv3 = self._upsample(256 + 256, 128)  # skip conn\n",
    "        self.upconv2 = self._upsample(128 + 128, 64)   # skip conn\n",
    "        self.upconv1 = self._upsample(64 + 64, 64)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def _upsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder_relu(self.encoder_bn1(self.encoder_conv1(x)))  # [B, 64, H/2, W/2]\n",
    "        x2 = self.encoder_layer1(self.encoder_maxpool(x1))               # [B, 64, H/4, W/4]\n",
    "        x3 = self.encoder_layer2(x2)                                     # [B, 128, H/8, W/8]\n",
    "        x4 = self.encoder_layer3(x3)                                     # [B, 256, H/16, W/16]\n",
    "        x5 = self.encoder_layer4(x4)                                     # [B, 512, H/32, W/32]\n",
    "\n",
    "        # Decoder with U-Net style skip connections (at least 2 used: x4, x3)\n",
    "        d4 = self.upconv4(x5)                    # [B, 256, H/16, W/16]\n",
    "        d4 = torch.cat([d4, x4], dim=1)          # skip conn 1\n",
    "\n",
    "        d3 = self.upconv3(d4)                    # [B, 128, H/8, W/8]\n",
    "        d3 = torch.cat([d3, x3], dim=1)          # skip conn 2\n",
    "\n",
    "        d2 = self.upconv2(d3)                    # [B, 64, H/4, W/4]\n",
    "        d2 = torch.cat([d2, x2], dim=1)          # optional skip\n",
    "\n",
    "        d1 = self.upconv1(d2)                    # [B, 64, H/2, W/2]\n",
    "\n",
    "        out = F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_dl, device, criterion):\n",
    "    model.train()\n",
    "    curr_loss = 0.\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        curr_loss += loss.item()\n",
    "    return curr_loss / len(train_dl)\n",
    "\n",
    "def validate(model, val_dl, device, criterion):\n",
    "    model.eval()\n",
    "    curr_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            curr_loss += loss.item()\n",
    "    return curr_loss / len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, split it and create dataloaders\n",
    "dataset = LandUseDataset(\"/content/drive/MyDrive/image_label_dataset.npz\")\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "model = UNetResNet18(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_lr = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nRunning experiment with learning rate = {lr}\")\n",
    "\n",
    "    model = UNetResNet18(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    num_epochs = 10 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "        val_loss = validate(model, val_loader, device, criterion)\n",
    "        print(f\"Epoch {epoch + 1}:\\n\\tAverage Training Loss: {train_loss:.4f}\\n\\tAverage Validation Loss: {val_loss:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_lr = lr\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"Saved new best model.\")\n",
    "\n",
    "print(f\"\\nBest model was with learning rate = {best_lr} (val loss = {best_val_loss:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
