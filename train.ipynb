{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import random\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandUseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        data = np.load(path)\n",
    "        self.X = data['X']   # Shape: (N, C=13, H, W)\n",
    "        self.y = data['y']   # Shape: (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32)  # (C=13, H, W)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)     # (H, W)\n",
    "        \n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        \n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=13):\n",
    "        super(UNetResNet18, self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet18\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Override the first conv layer to accept 13 input channels\n",
    "        self.encoder_conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder_bn1 = resnet.bn1\n",
    "        self.encoder_relu = resnet.relu\n",
    "        self.encoder_maxpool = resnet.maxpool\n",
    "\n",
    "        # ResNet layers\n",
    "        self.encoder_layer1 = resnet.layer1  # 64 -> 64\n",
    "        self.encoder_layer2 = resnet.layer2  # 64 -> 128\n",
    "        self.encoder_layer3 = resnet.layer3  # 128 -> 256\n",
    "        self.encoder_layer4 = resnet.layer4  # 256 -> 512\n",
    "\n",
    "        # Decoder part (upsampling + skip connections)\n",
    "        self.upconv4 = self._upsample(512, 256)\n",
    "        self.upconv3 = self._upsample(256 + 256, 128)  # skip conn\n",
    "        self.upconv2 = self._upsample(128 + 128, 64)   # skip conn\n",
    "        self.upconv1 = self._upsample(64 + 64, 64)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def _upsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder_relu(self.encoder_bn1(self.encoder_conv1(x)))  # [B, 64, H/2, W/2]\n",
    "        x2 = self.encoder_layer1(self.encoder_maxpool(x1))               # [B, 64, H/4, W/4]\n",
    "        x3 = self.encoder_layer2(x2)                                     # [B, 128, H/8, W/8]\n",
    "        x4 = self.encoder_layer3(x3)                                     # [B, 256, H/16, W/16]\n",
    "        x5 = self.encoder_layer4(x4)                                     # [B, 512, H/32, W/32]\n",
    "\n",
    "        # Decoder with U-Net style skip connections (at least 2 used: x4, x3)\n",
    "        d4 = self.upconv4(x5)                    # [B, 256, H/16, W/16]\n",
    "        d4 = torch.cat([d4, x4], dim=1)          # skip conn 1\n",
    "\n",
    "        d3 = self.upconv3(d4)                    # [B, 128, H/8, W/8]\n",
    "        d3 = torch.cat([d3, x3], dim=1)          # skip conn 2\n",
    "\n",
    "        d2 = self.upconv2(d3)                    # [B, 64, H/4, W/4]\n",
    "        d2 = torch.cat([d2, x2], dim=1)          # optional skip\n",
    "\n",
    "        d1 = self.upconv1(d2)                    # [B, 64, H/2, W/2]\n",
    "\n",
    "        out = F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_dl, device, criterion):\n",
    "    model.train()\n",
    "    curr_loss = 0.\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        curr_loss += loss.item()\n",
    "    return curr_loss / len(train_dl)\n",
    "\n",
    "def validate(model, val_dl, device, criterion):\n",
    "    model.eval()\n",
    "    curr_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            curr_loss += loss.item()\n",
    "    return curr_loss / len(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomApplyTransform:\n",
    "    def __init__(self, transform, p=0.5):\n",
    "        self.transform = transform\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.transform(x)\n",
    "        return x\n",
    "\n",
    "class RandomRotationTensor:\n",
    "    def __init__(self, degrees=15):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TF.rotate(x, angle)\n",
    "\n",
    "class RandomRadiometricShift:\n",
    "    def __init__(self, scale=0.05):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        shift = torch.empty_like(x).uniform_(-self.scale, self.scale)\n",
    "        return x + shift\n",
    "\n",
    "class RandomGaussianBlur:\n",
    "    def __init__(self, sigma_range=(0.5, 1.0)):\n",
    "        self.sigma_range = sigma_range\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(*self.sigma_range)\n",
    "        x_np = x.numpy()\n",
    "        x_np = gaussian_filter(x_np, sigma=(0, 1, 1))  # blur spatial dims\n",
    "        return torch.from_numpy(x_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(path, batch_size=16, train_split=0.7, val_split=0.15, test_split=0.15, seed=42):\n",
    "    # Define transforms for training\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        RandomApplyTransform(RandomRotationTensor(degrees=15), p=0.5),\n",
    "        RandomApplyTransform(RandomRadiometricShift(scale=0.05), p=0.5),\n",
    "        RandomApplyTransform(RandomGaussianBlur(), p=0.5)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Create base dataset (shared source)\n",
    "    base_dataset = LandUseDataset(path)\n",
    "\n",
    "    # Split indices manually for reproducibility\n",
    "    total_size = len(base_dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size   = int(val_split * total_size)\n",
    "    test_size  = total_size - train_size - val_size\n",
    "\n",
    "    train_indices, val_indices, test_indices = random_split(\n",
    "        range(total_size),\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    # Create separate datasets for each split with appropriate transforms\n",
    "    train_dataset = LandUseDataset(path, transform=train_transform)\n",
    "    val_dataset   = LandUseDataset(path, transform=None)\n",
    "    test_dataset  = LandUseDataset(path, transform=None)\n",
    "\n",
    "    # Create subsets using precomputed indices\n",
    "    train_ds = Subset(train_dataset, train_indices)\n",
    "    val_ds   = Subset(val_dataset, val_indices)\n",
    "    test_ds  = Subset(test_dataset, test_indices)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return base_dataset, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_loader, val_loader, test_loader = get_dataloaders(\"/content/drive/MyDrive/image_label_dataset.npz\")\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "model = UNetResNet18(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_lr = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nRunning experiment with learning rate = {lr}\")\n",
    "\n",
    "    model = UNetResNet18(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "        val_loss = validate(model, val_loader, device, criterion)\n",
    "        print(f\"Epoch {epoch + 1}:\\n\\tAverage Training Loss: {train_loss:.4f}\\n\\tAverage Validation Loss: {val_loss:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_lr = lr\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"Saved new best model.\")\n",
    "\n",
    "print(f\"\\nBest model was with learning rate = {best_lr} (val loss = {best_val_loss:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
