{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import random\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassJaccardIndex\n",
    "from collections import Counter\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_distribution(dataset):\n",
    "    label_counts = Counter()\n",
    "    for _, mask in dataset:\n",
    "        labels, counts = np.unique(mask.numpy(), return_counts=True)\n",
    "        label_counts.update(dict(zip(labels, counts)))\n",
    "\n",
    "    # Sorting by class index\n",
    "    sorted_labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[l] for l in sorted_labels]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(sorted_labels, counts, tick_label=[f'Class {i}' for i in sorted_labels])\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Pixel Count')\n",
    "    plt.title('Label Distribution (Pixel-wise)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class LandUseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        data = np.load(path)\n",
    "        self.X = np.clip(data['X'] / 10000.0, 0.0, 1.0)   # Shape: (N, C=13, H, W)\n",
    "        self.y = data['y']   # Shape: (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32)  # (C=13, H, W)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)     # (H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        return X, y\n",
    "\"\"\"\n",
    "\n",
    "class LandUseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        data = np.load(path)\n",
    "        self.X = np.clip(data['X'] / 10000.0, 0.0, 1.0)   # Shape: (N, C=13, H, W)\n",
    "        self.y = self._remap_labels(data['y'])            # Shape: (N, H, W)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _remap_labels(self, y):\n",
    "        id2idx = {10: 0, 20: 1, 30: 2, 40: 3, 50: 4, 60: 5, 80: 6, 90: 7}\n",
    "        y_remapped = np.copy(y)\n",
    "        for old, new in id2idx.items():\n",
    "            y_remapped[y == old] = new\n",
    "        return y_remapped\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X[idx], dtype=torch.float32)  # (C=13, H, W)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)     # (H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=13):\n",
    "        super(UNetResNet18, self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet18\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Override the first conv layer to accept 13 input channels\n",
    "        self.encoder_conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder_bn1 = resnet.bn1\n",
    "        self.encoder_relu = resnet.relu\n",
    "        self.encoder_maxpool = resnet.maxpool\n",
    "\n",
    "        # ResNet layers\n",
    "        self.encoder_layer1 = resnet.layer1  # 64 -> 64\n",
    "        self.encoder_layer2 = resnet.layer2  # 64 -> 128\n",
    "        self.encoder_layer3 = resnet.layer3  # 128 -> 256\n",
    "        self.encoder_layer4 = resnet.layer4  # 256 -> 512\n",
    "\n",
    "        # Decoder part (upsampling + skip connections)\n",
    "        self.upconv4 = self._upsample(512, 256)\n",
    "        self.upconv3 = self._upsample(256 + 256, 128)  # skip conn\n",
    "        self.upconv2 = self._upsample(128 + 128, 64)   # skip conn\n",
    "        self.upconv1 = self._upsample(64 + 64, 64)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def _upsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder_relu(self.encoder_bn1(self.encoder_conv1(x)))  # [B, 64, H/2, W/2]\n",
    "        x2 = self.encoder_layer1(self.encoder_maxpool(x1))               # [B, 64, H/4, W/4]\n",
    "        x3 = self.encoder_layer2(x2)                                     # [B, 128, H/8, W/8]\n",
    "        x4 = self.encoder_layer3(x3)                                     # [B, 256, H/16, W/16]\n",
    "        x5 = self.encoder_layer4(x4)                                     # [B, 512, H/32, W/32]\n",
    "\n",
    "        # Decoder with U-Net style skip connections (at least 2 used: x4, x3)\n",
    "        d4 = self.upconv4(x5)                    # [B, 256, H/16, W/16]\n",
    "        d4 = torch.cat([d4, x4], dim=1)          # skip conn 1\n",
    "\n",
    "        d3 = self.upconv3(d4)                    # [B, 128, H/8, W/8]\n",
    "        d3 = torch.cat([d3, x3], dim=1)          # skip conn 2\n",
    "\n",
    "        d2 = self.upconv2(d3)                    # [B, 64, H/4, W/4]\n",
    "        d2 = torch.cat([d2, x2], dim=1)          # optional skip\n",
    "\n",
    "        d1 = self.upconv1(d2)                    # [B, 64, H/2, W/2]\n",
    "\n",
    "        out = F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=13):\n",
    "        super(UNetResNet50, self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet-50\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Replace the first conv layer to accept 13 input channels\n",
    "        self.encoder_conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder_bn1 = resnet.bn1\n",
    "        self.encoder_relu = resnet.relu\n",
    "        self.encoder_maxpool = resnet.maxpool\n",
    "\n",
    "        # ResNet-50 layers\n",
    "        self.encoder_layer1 = resnet.layer1  # 256\n",
    "        self.encoder_layer2 = resnet.layer2  # 512\n",
    "        self.encoder_layer3 = resnet.layer3  # 1024\n",
    "        self.encoder_layer4 = resnet.layer4  # 2048\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = self._upsample(2048, 1024)\n",
    "        self.upconv3 = self._upsample(1024 + 1024, 512)\n",
    "        self.upconv2 = self._upsample(512 + 512, 256)\n",
    "        self.upconv1 = self._upsample(256 + 256, 64)\n",
    "\n",
    "        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def _upsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder_relu(self.encoder_bn1(self.encoder_conv1(x)))  # [B, 64, H/2, W/2]\n",
    "        x2 = self.encoder_layer1(self.encoder_maxpool(x1))               # [B, 256, H/4, W/4]\n",
    "        x3 = self.encoder_layer2(x2)                                     # [B, 512, H/8, W/8]\n",
    "        x4 = self.encoder_layer3(x3)                                     # [B, 1024, H/16, W/16]\n",
    "        x5 = self.encoder_layer4(x4)                                     # [B, 2048, H/32, W/32]\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.upconv4(x5)                    # [B, 1024, H/16, W/16]\n",
    "        d4 = torch.cat([d4, x4], dim=1)\n",
    "\n",
    "        d3 = self.upconv3(d4)                    # [B, 512, H/8, W/8]\n",
    "        d3 = torch.cat([d3, x3], dim=1)\n",
    "\n",
    "        d2 = self.upconv2(d3)                    # [B, 256, H/4, W/4]\n",
    "        d2 = torch.cat([d2, x2], dim=1)\n",
    "\n",
    "        d1 = self.upconv1(d2)                    # [B, 64, H/2, W/2]\n",
    "\n",
    "        out = F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_dl, device, criterion):\n",
    "    model.train()\n",
    "    curr_loss = 0.\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        curr_loss += loss.item()\n",
    "    return curr_loss / len(train_dl)\n",
    "\n",
    "\"\"\"def validate(model, val_dl, device, criterion):\n",
    "    model.eval()\n",
    "    curr_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            curr_loss += loss.item()\n",
    "    return curr_loss / len(val_dl)\"\"\"\n",
    "\n",
    "def validate(model, val_dl, device, criterion, num_classes):\n",
    "    model.eval()\n",
    "    curr_loss = 0.\n",
    "\n",
    "    # Metrics\n",
    "    pixel_acc = MulticlassAccuracy(num_classes=num_classes, average='micro').to(device)\n",
    "    iou = MulticlassJaccardIndex(num_classes=num_classes, average=None).to(device)  # per-class IoU\n",
    "    mean_iou = MulticlassJaccardIndex(num_classes=num_classes, average='macro').to(device)  # mean IoU\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)  # shape: [B, C, H, W]\n",
    "            loss = criterion(logits, y)\n",
    "            curr_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)  # shape: [B, H, W]\n",
    "\n",
    "            # Flatten predictions and targets\n",
    "            preds_flat = preds.flatten()\n",
    "            y_flat = y.flatten()\n",
    "\n",
    "            # Mask: keep only valid class labels\n",
    "            mask = (y_flat >= 0) & (y_flat < num_classes)\n",
    "            preds_flat = preds_flat[mask]\n",
    "            y_flat = y_flat[mask]\n",
    "\n",
    "            # Update metrics\n",
    "            pixel_acc.update(preds.flatten(), y.flatten())\n",
    "            iou.update(preds.flatten(), y.flatten())\n",
    "            mean_iou.update(preds.flatten(), y.flatten())\n",
    "\n",
    "    # Compute final metric values\n",
    "    avg_loss = curr_loss / len(val_dl)\n",
    "    pixel_accuracy = pixel_acc.compute().item()\n",
    "    per_class_iou = iou.compute().cpu().numpy()  # shape: [num_classes]\n",
    "    mean_iou_val = mean_iou.compute().item()\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": avg_loss,\n",
    "        \"pixel_accuracy\": pixel_accuracy,\n",
    "        \"per_class_iou\": per_class_iou,\n",
    "        \"mean_iou\": mean_iou_val,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomApplyTransform:\n",
    "    def __init__(self, transform, p=0.5):\n",
    "        self.transform = transform\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.transform(x)\n",
    "        return x\n",
    "\n",
    "class RandomRotationTensor:\n",
    "    def __init__(self, degrees=15):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TF.rotate(x, angle)\n",
    "\n",
    "class RandomRadiometricShift:\n",
    "    def __init__(self, scale=0.05):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, x):\n",
    "        shift = torch.empty_like(x).uniform_(-self.scale, self.scale)\n",
    "        return x + shift\n",
    "\n",
    "class RandomGaussianBlur:\n",
    "    def __init__(self, sigma_range=(0.5, 1.0)):\n",
    "        self.sigma_range = sigma_range\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(*self.sigma_range)\n",
    "        x_np = x.numpy()\n",
    "        x_np = gaussian_filter(x_np, sigma=(0, 1, 1))  # blur spatial dims\n",
    "        return torch.from_numpy(x_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(path, batch_size=16, train_split=0.7, val_split=0.15, test_split=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Creates and returns dataloaders for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the .npz file containing the dataset\n",
    "        batch_size (int): Batch size for the dataloaders\n",
    "        train_split (float): Proportion of data to use for training\n",
    "        val_split (float): Proportion of data to use for validation\n",
    "        test_split (float): Proportion of data to use for testing\n",
    "        seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dataset, train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Define transforms for training\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(p=0.4),\n",
    "        #RandomApplyTransform(RandomRotationTensor(degrees=15), p=0.5),\n",
    "        RandomApplyTransform(RandomRadiometricShift(scale=0.05), p=0.4),\n",
    "        #RandomApplyTransform(RandomGaussianBlur(), p=0.2)\n",
    "    ])\n",
    "\n",
    "    # Create a single dataset instance\n",
    "    dataset = LandUseDataset(path)\n",
    "\n",
    "    # Split indices manually for reproducibility\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_split * total_size)\n",
    "\n",
    "    indices = list(range(total_size))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "\n",
    "    # Create custom dataset wrappers that apply transforms on-the-fly\n",
    "    class TransformSubset(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, indices, transform=None):\n",
    "            self.dataset = dataset\n",
    "            self.indices = indices\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            X, y = self.dataset[self.indices[idx]]\n",
    "            if self.transform:\n",
    "                X = self.transform(X)\n",
    "            return X, y\n",
    "\n",
    "    # Create subsets with appropriate transforms\n",
    "    train_ds = TransformSubset(dataset, train_indices, transform=train_transform)\n",
    "    val_ds = TransformSubset(dataset, val_indices, transform=None)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,  \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataset, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset, train_loader, val_loader = get_dataloaders(\"/content/drive/MyDrive/image_label_dataset.npz\")\n",
    "dataset, train_loader, val_loader = get_dataloaders(\"/content/drive/MyDrive/generated_dataset.npz\")\n",
    "num_classes = len(np.unique(dataset.y))\n",
    "print(\"Unique labels in masks:\", np.unique(dataset.y))\n",
    "plot_label_distribution(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetResNet18(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = 10000.\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "    val_metrics = validate(model, val_loader, device, criterion, num_classes)\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    print(f\"Average Training Loss {train_loss}\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    scheduler.step(val_metrics[\"val_loss\"])\n",
    "\n",
    "    if val_metrics[\"val_loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"val_loss\"]\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "print(f\"\\nBest model was with(val loss = {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetResNet50(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = 10000.\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "    val_metrics = validate(model, val_loader, device, criterion, num_classes)\n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    print(f\"Average Training Loss {train_loss}\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    scheduler.step(val_metrics[\"val_loss\"])\n",
    "\n",
    "    if val_metrics[\"val_loss\"] < best_val_loss:\n",
    "        best_val_loss = val_metrics[\"val_loss\"]\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "print(f\"\\nBest model was with(val loss = {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_lr = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nRunning experiment with learning rate = {lr}\")\n",
    "\n",
    "    model = UNetResNet18(input_channels=13, num_classes=int(dataset.y.max()) + 1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    num_epochs = 30\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, criterion)\n",
    "        val_loss = validate(model, val_loader, device, criterion)\n",
    "        print(f\"Epoch {epoch + 1}:\\n\\tAverage Training Loss: {train_loss:.4f}\\n\\tAverage Validation Loss: {val_loss:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_lr = lr\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"Saved new best model.\")\n",
    "\n",
    "print(f\"\\nBest model was with learning rate = {best_lr} (val loss = {best_val_loss:.4f})\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
